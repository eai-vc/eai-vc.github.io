<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Visual Cortex (VC-1)</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./assets/images/vc1_logo.png">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Where are we in the search for an Artificial Visual Cortex
                            for Embodied Intelligence?</h1>                    
                <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://arjunmajum.github.io">Arjun&#160;Majumdar</a><sup>1&star;</sup>,
                <a target="_blank" href="http://karmeshyadav.com/">Karmesh&#160;Yadav</a><sup>2&star;</sup>,
                <a target="_blank"href="https://www.linkedin.com/in/sergio-arnaud-226456198/">Sergio&#160;Arnaud</a><sup>2&star;</sup>,
                <a target="_blank" href="https://www.seas.upenn.edu/~jasonyma/">Jason&#160;Ma</a><sup>3</sup>,
                <a target="_blank" href="https://msl.stanford.edu/people/clairechen">Claire&#160;Chen</a><sup>4</sup>,
                <a target="_blank" href="http://ssilwal.com/">Sneha&#160;Silwal</a><sup>2</sup>,
                <a target="_blank" href="https://aryan9101.github.io/">Aryan&#160;Jain</a><sup>5</sup>,
                <a target="_blank" href="https://www.linkedin.com/in/vincentpierreberges/">Vincent-Pierre&#160;Berges</a><sup>2</sup>,
                <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter&#160;Abbeel</a><sup>5</sup>,
                <a target="_blank" href="https://people.eecs.berkeley.edu/~malik/">Jitendra&#160;Malik</a><sup>5 2</sup>,
                <a target="_blank" href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv&#160;Batra</a><sup>1 2</sup>,
                <a target="_blank" href="https://yixinlin.net/">Yixin&#160;Lin</a><sup>2&dagger;</sup>,
                <a target="_blank" href="https://www.maksymets.com/">Oleksandr&#160;Maksymets</a><sup>2&dagger;</sup>,
                <a target="_blank" href="https://aravindr93.github.io/">Aravind&#160;Rajeswaran</a><sup>2&dagger;</sup>,
                <a target="_blank" href="https://fmeier.github.io">Franziska&#160;Meier</a><sup>2&dagger;</sup>

            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Georgia Institute of Technology, </span>
                        <span class="author-block"><sup>2</sup>Meta AI, </span>
                        <span class="author-block"><sup>3</sup>University of Pennsylvania, </span>
                        <span class="author-block"><sup>4</sup>Stanford University, </span>
                        <span class="author-block"><sup>5</sup>UC Berkeley </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>&star;</sup>Equal Contribution</span>
                        <!-- Add 5 spaces -->
                        <span class="author-block">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
                        <span class="author-block"><sup>&dagger;</sup>Equal Contribution</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2303.18240"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                            <span class="link-block">
                <a target="_blank" href="https://ai.facebook.com/blog/robots-learning-video-simulation-artificial-visual-cortex-vc-1/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-blog"></i>
                  </span>
                  <span>Blog</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/facebookresearch/eai-vc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://github.com/facebookresearch/eai-vc/tree/main/MODEL_CARD.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-network-wired"></i>
                  </span>
                  <span>Models</span>
                </a>
                <a target="_blank" href="https://github.com/facebookresearch/eai-vc/tree/main/cortexbench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>Benchmark</span>
                </a>
                <a target="_blank" href="https://github.com/facebookresearch/eai-vc/blob/main/cortexbench/DATASETS.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <img src="assets/images/vc1_teaser.gif" class="interpolation-image"
                alt="" style="display: block; margin-left: auto; margin-right: auto"
                width="80%">
                <span style="font-size: 100%">
                We assemble <b>CortexBench</b> from 7 benchmarks and systematically evaluate existing visual representation models. We then train a single new model 
                <b>Visual Cortex-1 (VC-1)</b>, compare it to the best prior result on each benchmark (above), and adapt it to specific domains.
                </span>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual ‘foundation models’ for
                        Embodied AI. First, we curate <b>CortexBench</b>, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation.
                        Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data scale and
                        diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized
                        vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset
                        size and diversity does not improve performance universally (but does so on average). Our largest model, named <b>VC-1</b>, outperforms all prior PVRs on
                        average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantial gains, with
                        VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. These models required
                        over 10,000 GPU-hours to train and can be found on our website for the benefit of the research community.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                        class="dvc1">Overview</span></h2>
                    <img src="assets/images/cortexbench_overview.png" class="interpolation-image"
                    alt="" style="display: block; margin-left: auto; margin-right: auto"
                        width="100%">
                    <span style="font-size: 110%">
                    <span style="font-weight: bold">
                        Overview of CortexBench.</span> We assemble relevant datasets and visual representation learning algorithms to produce 
                        candidate Visual Cortex models, which are then evaluated using either reinforcement or imitation learning on a set of highly diverse tasks.
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--CortexBench-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvc1">Evaluating Existing Models</span></h2>
                    <div class="content has-text-justified">
                    <!-- Put the image on right and text on left -->
                    <img src="assets/images/baselines_rank_distribution.png" class="click here"
                         alt="" style="float: right; margin-right: 15px;"
                            width="45%">
                    <span style="font-size: 125% ">
                        <!-- justify text -->
                            We evaluated several pre-trained visual representations (PVRs) on CortexBench to assess their consistent performance across tasks. The models
                            included CLIP, R3M, MVP, and VIP, representing different architectures, pre-training objectives, and datasets. We also examined the necessity
                            of pre-training and the limitations of end-to-end learning. Our findings indicate that no single model excelled in all tasks, with R3M performing
                            best on Adroit, MetaWorld, and DMControl, MVP (ViT-L) on Trifinger, ImageNav, and Mobile Pick, and CLIP on ObjectNav. These results demonstrate
                            the variance in performance of existing PVRs on CortexBench and underscore the lack of a single, strong-performing artificial visual cortex for embodied AI.

                    </div>
                    </span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvc1">Experiments</span></h2>

                    <p style="font-size: 125%">
                        After discovering that no single artificial visual cortex exists yet, we conduct our own investigations by building a pretrained visual encoder. We fix the pre-training objective – MAE and then vary the composition of the pre-training dataset and the size of the visual backbone (ViT-B with 86M parameters and ViT-L with 307M parameters). We conduct our experiments on CortexBench and try to answer three main questions:
                    <ul style="font-size: 125%; padding-left: 5%">
                        <li>(1) What is the impact of scaling dataset size and diversity?
                        </li>
                        <li>(2) How does the inclusion of less-relevant datasets influence
                            the performance of PVRs on embodied AI tasks?
                        </li>
                        <li>(3) How does VC-1 compare to existing PVRs?
                        </li>

                    </ul>
                    </p>
                    <br>
                    <br>

                    <div class="columns">
                        <div class="column has-text-centered">
                            <img src="assets/images/scaling_model.jpg" class="interpolation-image"
                                alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <span style="font-size: 100%">
                                <span style="font-weight: bold">Scaling Dataset Size</span>
                            </span>
                        </div>
                        <div class="column has-text-centered">
                            <img src="assets/images/scaling_dataset.jpg" class="interpolation-image"
                                alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <span style="font-size: 100%">
                                <span style="font-weight: bold">Scaling Model Size</span>
                            </span>
                        </div>
                        <div class="column has-text-centered">
                            <img src="assets/images/benchmark_rank_plot.jpg" class="interpolation-image"
                                alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                            <span style="font-size: 100%">
                                <span style="font-weight: bold">Ranking of all Models</span>
                            </span>
                        </div>
                    </div>
                    <span style="font-size: 100%">
                        <span style="font-weight: bold">Scaling experiments:</span> Visualizing model performance averaged across all benchmarks.
                        Overall, we demonstrate modest but positive scaling trends in both (a) scaling model size, and (b) dataset diversity. c) Average ranking across all benchmarks.
                        We compare existing PVR models and scaling models by showcasing their ranking across all benchmarks, VC-1 (MAE Ego4D+MNI) achieves the highest average rank.
                    </span>
            </div>

        </div>
    </div>
</section>

<!--Adaptation-->
<section class="adaptation">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h3 class="title is-3"><span
                            class="dvc1">Adapting VC-1</span></h3>
                        <p style="font-size: 125%">
                            Finally, we study if adapting VC-1 can lead to improved results on CortexBench. We believe adaptation can be useful for at least these two reasons:
                        <ul style="font-size: 125%; padding-left: 5%">
                            <li>(1) Since VC-1 was trained with MAE, it captures features that are generally useful for reconstructing images. Adaptation can specialize the
                                visual backbone to extract features required for performing specific downstream tasks such as object rearrangement.
                            </li>
                            <li>(2) Adaptation can also help mitigate domain-gap that might exist between pre-training and evaluation settings. Domain gap specifically exists in our setup since VC-1 
                                was pretrained with human video data while our downstream evaluation in CortexBench uses simulated tasks.
                            </li>
                        </ul>
                        <br>
                        <p style="font-size: 125%">
                            We try the following adaptation strategies:
                            <ul style="font-size: 125%; padding-left: 5%">
                                <li>(1) <b>End-to-end (E2E) fine-tuning</b> using a task-specific loss function </li>
                                <li>(2) <b>MAE adaptation</b> using task specific data  </li>
                            </ul>
                           
                    <br>                    

                    <img src="assets/images/adaptation_plot.jpg" class="interpolation-image"
                        alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold"> Adaptation Plots: </span> Adapting VC-1 with end-to-end fine-tuning or self-supervised learning (MAE) on in-domain data leads to substantial gains in performance.
                    </span>

                    <br>
                    <br>

                </div>
            </div>
        </div>
    </div>
</section>

<section class="adaptation">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h3 class="title is-3"><span
                            class="dvc1">Interpreting VC-1</span></h3>
                    <span style="font-size: 125% ">
                        We investigate the visual representations of VC-1 by visualizing the last attention layer of the pretrained Vision Transformer. This enables us to obtain a better understanding of the regions of the image that receive the most weight in the learned representation, just before it is passed downstream to the policy. Our analysis indicates that models trained using masked auto-encoding tend to prioritize edges, boundaries, and complex visual features in the scene. Interestingly, once we finetune VC-1, the attention gets localized specifically to the region of interest and as the scene progresses, the model's attention shifts from the surrounding area of the object to the object itself and ultimately to the end-effector. 
                    <br>
                    <br>

                    <div class="columns">
                        <div class="column has-text-centered">
                            <h3 class="title is-5">Frozen Model Attention Visualization</h3>
                            <video id="frozen_adaptation"
                                poster=""
                                autoplay
                                controls
                                muted
                                loop
                                height="100%"
                                playbackRate=2.0>
                                <source src="assets/videos/frozen_vc1_mobilepick.mp4"
                                type="video/mp4">
                            </video>
                        </div>

                        <div class="column has-text-centered">
                            <h3 class="title is-5">Finetuned Model Attention Visualization</h3>
                            <video id="frozen_adaptation"
                                poster=""
                                autoplay
                                controls
                                muted
                                loop
                                height="100%"
                                playbackRate=2.0>
                                <source src="assets/videos/e2e_finetuned_vc1_mobilepick.mp4"
                                type="video/mp4">
                            </video>
                        </div>
                    </div>

                </div>
            </div>
        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvc1">Conclusion</span></h2>
                    <div class="content has-text-justified">

                    <p style="font-size: 125%">
                        This work introduced CortexBench, which comprises of 17 different embodied AI (EAI) task spanning locomotion,
                        indoor navigation, and dexterous and mobile manipulation. Enabled by CortexBench, we performed the most comprehensive
                        study to-date of visual foundation models for EAI. Specifically, we evaluated state-of-art open-sourced foundation models and find that we do not yet have a strong
                        backbone for all tasks. However, models trained via masked auto-encoders (MAEs) are the most promising. Our study also
                        finds that naively scaling model size and pretraining data diversity does not improve performance universally
                        across all tasks, but does so on average. Finally, we find that adapting our largest pre-trained model (VC-1) results
                        in performance that is competitive with or outperforms the best known results on all benchmarks in CortexBench.
                    </p>

                    <p style="font-size: 125%">
                        One of our primary contentions is that in order for the research community to make progress on foundation models for 
                        EAI, we need to develop strong benchmarks – for a PVR to be foundational, it must be broadly applicable. Furthermore, 
                        as a community we should converge on best practices and strive towards a rigorous reproducible experimental methodology; we hope 
                        CortexBench will help the community make progress towards that.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
    @inproceedings{vc2023,
        title         = {Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?}, 
        author        = {Arjun Majumdar and Karmesh Yadav and Sergio Arnaud and Yecheng Jason Ma and Claire Chen and
                         Sneha Silwal and Aryan Jain and Vincent-Pierre Berges and Pieter Abbeel and Jitendra Malik and
                         Dhruv Batra and Yixin Lin and Oleksandr Maksymets and Aravind Rajeswaran and Franziska Meier},
        year          = {2023},
        eprint        = {2303.18240},
        archivePrefix = {arXiv},
        primaryClass  = {cs.CV}
    }
        </code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                            href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
